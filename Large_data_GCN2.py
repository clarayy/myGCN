import os.path as osp
from math import ceil
from scipy.sparse.coo import coo_matrix
import torch
import torch.nn.functional as F
import os
import torch_geometric.transforms as T
from MyData_fromTU import MyData_fromTU
from MyOwnData import MyOwnDataset
from torch_geometric.loader import DenseDataLoader
from torch_geometric.nn import DenseSAGEConv, dense_diff_pool
import numpy as np
import cv2
from torchvision import transforms, utils
from torch.utils.data import Dataset, DataLoader
from torch_geometric.data import Data
from typing import Union, List

from torch.utils.data.dataloader import default_collate

from torch_geometric.data import Data, Batch, Dataset

torch.set_printoptions(profile='full')
class0= {3: 0, 5: 1, 9: 2, 28: 3, 31: 4, 53: 5, 56: 6, 68: 7, 75: 8, 77: 9, 81: 10, 87: 11, 90: 12, 92: 13, 93: 14, 96: 15, 98: 16, 103: 17, 105: 18, 109: 19, 117: 20, 138: 21, 139: 22, 144: 23, 158: 24, 160: 25, 165: 26, 174: 27, 177: 28, 188: 29, 189: 30, 192: 31, 194: 32, 196: 33, 201: 34, 218: 35, 221: 36, 227: 37, 230: 38, 240: 39, 244: 40, 248: 41, 251: 42, 252: 43, 256: 44, 257: 45, 268: 46, 269: 47, 272: 48, 282: 49, 283: 50, 289: 51, 296: 52, 299: 53, 300: 54, 306: 55, 307: 56, 320: 57, 324: 58, 338: 59, 339: 60, 341: 61, 351: 62, 352: 63, 356: 64, 361: 65, 364: 66, 365: 67, 367: 68, 386: 69, 389: 70, 390: 71, 395: 72, 402: 73, 412: 74, 415: 75, 419: 76, 420: 77, 421: 78, 429: 79, 434: 80, 439: 81, 442: 82, 446: 83, 451: 84, 453: 85, 456: 86, 458: 87, 461: 88, 464: 89, 468: 90, 469: 91, 477: 92, 479: 93, 483: 94, 484: 95, 495: 96, 497: 97, 501: 98, 502: 99, 507: 100, 509: 101, 512: 102, 515: 103, 520: 104, 521: 105, 524: 106, 525: 107, 530: 108, 532: 109, 535: 110, 537: 111, 538: 112, 540: 113, 544: 114, 551: 115, 557: 116, 560: 117, 561: 118, 564: 119, 566: 120, 580: 121, 581: 122, 596: 123, 605: 124, 611: 125, 615: 126, 621: 127, 624: 128, 625: 129, 630: 130, 637: 131, 638: 132, 645: 133, 648: 134, 653: 135, 656: 136, 660: 137, 686: 138, 687: 139, 696: 140, 700: 141, 707: 142, 714: 143, 723: 144, 724: 145, 734: 146, 738: 147, 741: 148, 768: 149, 776: 150, 782: 151, 784: 152, 792: 153, 793: 154, 794: 155, 799: 156, 810: 157, 821: 158, 827: 159, 830: 160, 834: 161, 853: 162, 857: 163, 858: 164, 859: 165, 862: 166, 867: 167, 876: 168, 877: 169, 884: 170, 885: 171, 886: 172, 887: 173, 888: 174, 924: 175, 926: 176, 927: 177, 929: 178, 931: 179, 937: 180, 940: 181, 952: 182, 953: 183, 961: 184, 970: 185, 978: 186, 979: 187, 983: 188, 985: 189, 994: 190, 1001: 191, 1003: 192, 1007: 193, 1025: 194, 1034: 195, 1043: 196, 1044: 197, 1047: 198, 1048: 199, 1054: 200, 1061: 201, 1069: 202, 1070: 203, 1082: 204, 1097: 205, 1099: 206, 1103: 207, 1104: 208, 1125: 209, 1127: 210, 1129: 211, 1130: 212, 1135: 213, 1137: 214, 1138: 215, 1145: 216, 1154: 217, 1165: 218, 1167: 219, 1168: 220, 1174: 221, 1177: 222, 1187: 223, 1191: 224, 1193: 225, 1203: 226, 1206: 227, 1208: 228, 1215: 229, 1220: 230, 1224: 231, 1226: 232, 1229: 233, 1235: 234, 1244: 235, 1251: 236, 1264: 237, 1266: 238, 1273: 239, 1275: 240, 1276: 241, 1282: 242, 1286: 243, 1297: 244, 1299: 245, 1301: 246, 1310: 247, 1316: 248, 1318: 249, 1320: 250, 1324: 251, 1330: 252, 1331: 253, 1341: 254, 1344: 255, 1354: 256, 1359: 257, 1360: 258, 1368: 259, 1371: 260, 1372: 261, 1373: 262, 1382: 263, 1386: 264, 1390: 265, 1395: 266, 1404: 267, 1407: 268, 1408: 269, 1411: 270, 1412: 271, 1413: 272, 1421: 273, 1423: 274, 1432: 275, 1439: 276, 1447: 277, 1462: 278, 1463: 279, 1465: 280, 1476: 281, 1477: 282, 1480: 283, 1493: 284, 1495: 285, 1496: 286, 1498: 287, 1508: 288, 1517: 289, 1523: 290, 1524: 291, 1531: 292, 1553: 293, 1560: 294, 1571: 295, 1572: 296, 1576: 297, 1585: 298, 1587: 299, 1589: 300, 1591: 301, 1597: 302, 1599: 303, 1606: 304, 1612: 305, 1614: 306, 1618: 307, 1620: 308, 1621: 309, 1626: 310, 1628: 311, 1630: 312, 1632: 313, 1636: 314, 1640: 315, 1643: 316, 1650: 317, 1658: 318, 1670: 319, 1678: 320, 1680: 321, 1682: 322, 1683: 323, 1686: 324, 1687: 325, 1693: 326, 1695: 327, 1699: 328, 1702: 329, 1707: 330, 1708: 331, 1713: 332, 1729: 333, 1730: 334, 1735: 335, 1745: 336, 1748: 337, 1759: 338, 1763: 339, 1769: 340, 1770: 341, 1773: 342, 1776: 343, 1791: 344, 1807: 345, 1809: 346, 1814: 347, 1828: 348, 1831: 349, 1835: 350, 1840: 351, 1844: 352, 1853: 353, 1858: 354, 1859: 355, 1860: 356, 1873: 357, 1887: 358, 1889: 359, 1890: 360, 1891: 361, 1893: 362, 1900: 363, 1903: 364, 1910: 365, 1911: 366, 1921: 367, 1925: 368, 1931: 369, 1933: 370, 1934: 371, 1935: 372, 1938: 373, 1941: 374, 1947: 375, 1955: 376, 1963: 377, 1964: 378, 1969: 379, 1981: 380, 1983: 381, 1984: 382, 1989: 383, 1993: 384, 1994: 385, 2004: 386, 2009: 387, 2013: 388, 2015: 389, 2016: 390, 2019: 391, 2027: 392, 2033: 393, 2044: 394, 2050: 395, 2053: 396, 2054: 397, 2056: 398, 2061: 399}
class1= {14: 0, 29: 1, 33: 2, 34: 3, 37: 4, 39: 5, 54: 6, 58: 7, 61: 8, 65: 9, 71: 10, 83: 11, 91: 12, 99: 13, 113: 14, 114: 15, 115: 16, 116: 17, 126: 18, 133: 19, 135: 20, 136: 21, 146: 22, 159: 23, 168: 24, 169: 25, 175: 26, 183: 27, 185: 28, 204: 29, 209: 30, 216: 31, 217: 32, 226: 33, 229: 34, 233: 35, 234: 36, 235: 37, 238: 38, 242: 39, 246: 40, 249: 41, 250: 42, 254: 43, 258: 44, 263: 45, 321: 46, 322: 47, 323: 48, 333: 49, 335: 50, 337: 51, 344: 52, 346: 53, 350: 54, 353: 55, 363: 56, 369: 57, 371: 58, 372: 59, 377: 60, 387: 61, 396: 62, 397: 63, 401: 64, 404: 65, 405: 66, 410: 67, 414: 68, 417: 69, 425: 70, 430: 71, 433: 72, 435: 73, 440: 74, 443: 75, 447: 76, 467: 77, 470: 78, 475: 79, 487: 80, 494: 81, 496: 82, 498: 83, 513: 84, 516: 85, 522: 86, 528: 87, 556: 88, 565: 89, 574: 90, 595: 91, 597: 92, 608: 93, 610: 94, 620: 95, 623: 96, 628: 97, 634: 98, 635: 99, 644: 100, 646: 101, 650: 102, 654: 103, 661: 104, 662: 105, 664: 106, 669: 107, 675: 108, 676: 109, 678: 110, 679: 111, 683: 112, 690: 113, 694: 114, 699: 115, 704: 116, 705: 117, 711: 118, 722: 119, 743: 120, 750: 121, 753: 122, 760: 123, 763: 124, 769: 125, 774: 126, 775: 127, 786: 128, 788: 129, 804: 130, 805: 131, 815: 132, 822: 133, 825: 134, 841: 135, 843: 136, 845: 137, 847: 138, 851: 139, 861: 140, 865: 141, 875: 142, 892: 143, 896: 144, 903: 145, 908: 146, 914: 147, 915: 148, 919: 149, 920: 150, 921: 151, 939: 152, 946: 153, 948: 154, 949: 155, 968: 156, 1002: 157, 1010: 158, 1017: 159, 1029: 160, 1030: 161, 1037: 162, 1045: 163, 1050: 164, 1052: 165, 1058: 166, 1066: 167, 1075: 168, 1088: 169, 1092: 170, 1102: 171, 1112: 172, 1121: 173, 1147: 174, 1151: 175, 1153: 176, 1155: 177, 1161: 178, 1175: 179, 1180: 180, 1189: 181, 1195: 182, 1197: 183, 1204: 184, 1205: 185, 1217: 186, 1218: 187, 1222: 188, 1247: 189, 1258: 190, 1305: 191, 1307: 192, 1329: 193, 1339: 194, 1347: 195, 1356: 196, 1357: 197, 1376: 198, 1388: 199, 1389: 200, 1392: 201, 1399: 202, 1401: 203, 1424: 204, 1426: 205, 1436: 206, 1438: 207, 1445: 208, 1448: 209, 1455: 210, 1456: 211, 1467: 212, 1472: 213, 1474: 214, 1478: 215, 1482: 216, 1490: 217, 1492: 218, 1500: 219, 1505: 220, 1506: 221, 1516: 222, 1543: 223, 1544: 224, 1548: 225, 1549: 226, 1551: 227, 1554: 228, 1558: 229, 1565: 230, 1588: 231, 1590: 232, 1595: 233, 1609: 234, 1619: 235, 1638: 236, 1641: 237, 1642: 238, 1644: 239, 1654: 240, 1656: 241, 1660: 242, 1661: 243, 1672: 244, 1673: 245, 1675: 246, 1676: 247, 1679: 248, 1691: 249, 1696: 250, 1714: 251, 1720: 252, 1738: 253, 1742: 254, 1751: 255, 1795: 256, 1806: 257, 1812: 258, 1824: 259, 1834: 260, 1837: 261, 1845: 262, 1849: 263, 1850: 264, 1864: 265, 1872: 266, 1876: 267, 1880: 268, 1884: 269, 1885: 270, 1897: 271, 1898: 272, 1915: 273, 1922: 274, 1950: 275, 1953: 276, 1959: 277, 1967: 278, 1968: 279, 1973: 280, 1976: 281, 1977: 282, 2012: 283, 2024: 284, 2035: 285, 2036: 286, 2057: 287, 2058: 288, 2065: 289}
class2= {42: 0, 62: 1, 64: 2, 80: 3, 111: 4, 119: 5, 143: 6, 161: 7, 171: 8, 176: 9, 191: 10, 197: 11, 219: 12, 231: 13, 243: 14, 260: 15, 265: 16, 280: 17, 297: 18, 302: 19, 317: 20, 318: 21, 319: 22, 360: 23, 373: 24, 376: 25, 378: 26, 406: 27, 407: 28, 416: 29, 418: 30, 422: 31, 431: 32, 460: 33, 463: 34, 472: 35, 478: 36, 482: 37, 486: 38, 489: 39, 510: 40, 514: 41, 541: 42, 559: 43, 573: 44, 577: 45, 585: 46, 588: 47, 589: 48, 604: 49, 606: 50, 609: 51, 633: 52, 640: 53, 641: 54, 642: 55, 647: 56, 670: 57, 709: 58, 713: 59, 715: 60, 730: 61, 735: 62, 737: 63, 767: 64, 789: 65, 809: 66, 811: 67, 816: 68, 828: 69, 833: 70, 842: 71, 848: 72, 849: 73, 866: 74, 868: 75, 869: 76, 878: 77, 883: 78, 889: 79, 911: 80, 917: 81, 928: 82, 936: 83, 942: 84, 944: 85, 959: 86, 962: 87, 969: 88, 990: 89, 1005: 90, 1006: 91, 1011: 92, 1019: 93, 1023: 94, 1041: 95, 1049: 96, 1053: 97, 1060: 98, 1071: 99, 1084: 100, 1090: 101, 1095: 102, 1100: 103, 1115: 104, 1119: 105, 1132: 106, 1133: 107, 1157: 108, 1159: 109, 1188: 110, 1192: 111, 1212: 112, 1219: 113, 1221: 114, 1232: 115, 1239: 116, 1249: 117, 1250: 118, 1254: 119, 1257: 120, 1262: 121, 1265: 122, 1283: 123, 1295: 124, 1309: 125, 1313: 126, 1314: 127, 1315: 128, 1332: 129, 1334: 130, 1338: 131, 1345: 132, 1346: 133, 1370: 134, 1380: 135, 1396: 136, 1398: 137, 1409: 138, 1425: 139, 1443: 140, 1460: 141, 1479: 142, 1483: 143, 1494: 144, 1503: 145, 1514: 146, 1533: 147, 1547: 148, 1550: 149, 1555: 150, 1559: 151, 1568: 152, 1573: 153, 1578: 154, 1580: 155, 1594: 156, 1601: 157, 1623: 158, 1637: 159, 1639: 160, 1645: 161, 1651: 162, 1689: 163, 1690: 164, 1703: 165, 1704: 166, 1717: 167, 1722: 168, 1750: 169, 1760: 170, 1762: 171, 1772: 172, 1777: 173, 1783: 174, 1785: 175, 1792: 176, 1799: 177, 1822: 178, 1829: 179, 1836: 180, 1843: 181, 1870: 182, 1871: 183, 1878: 184, 1883: 185, 1896: 186, 1930: 187, 1948: 188, 1971: 189, 1972: 190, 1978: 191, 1982: 192, 1986: 193, 1988: 194, 1990: 195, 2001: 196, 2005: 197, 2022: 198, 2032: 199, 2037: 200, 2045: 201, 2049: 202, 2066: 203}
class3= {10: 0, 18: 1, 24: 2, 26: 3, 36: 4, 59: 5, 63: 6, 88: 7, 97: 8, 120: 9, 121: 10, 129: 11, 130: 12, 131: 13, 153: 14, 157: 15, 162: 16, 164: 17, 178: 18, 187: 19, 195: 20, 210: 21, 211: 22, 212: 23, 220: 24, 225: 25, 236: 26, 266: 27, 271: 28, 281: 29, 284: 30, 293: 31, 303: 32, 304: 33, 325: 34, 327: 35, 332: 36, 334: 37, 336: 38, 359: 39, 366: 40, 370: 41, 380: 42, 393: 43, 394: 44, 399: 45, 413: 46, 428: 47, 432: 48, 450: 49, 454: 50, 457: 51, 474: 52, 492: 53, 499: 54, 518: 55, 533: 56, 546: 57, 558: 58, 570: 59, 572: 60, 576: 61, 579: 62, 583: 63, 614: 64, 616: 65, 619: 66, 636: 67, 639: 68, 643: 69, 651: 70, 657: 71, 658: 72, 659: 73, 665: 74, 667: 75, 672: 76, 673: 77, 682: 78, 716: 79, 719: 80, 739: 81, 740: 82, 744: 83, 751: 84, 758: 85, 762: 86, 764: 87, 773: 88, 785: 89, 787: 90, 797: 91, 812: 92, 826: 93, 835: 94, 836: 95, 850: 96, 852: 97, 854: 98, 860: 99, 872: 100, 879: 101, 882: 102, 890: 103, 902: 104, 904: 105, 906: 106, 923: 107, 947: 108, 954: 109, 958: 110, 971: 111, 981: 112, 982: 113, 1014: 114, 1016: 115, 1022: 116, 1036: 117, 1040: 118, 1042: 119, 1051: 120, 1057: 121, 1064: 122, 1068: 123, 1072: 124, 1083: 125, 1085: 126, 1094: 127, 1096: 128, 1108: 129, 1111: 130, 1120: 131, 1144: 132, 1162: 133, 1173: 134, 1186: 135, 1190: 136, 1234: 137, 1236: 138, 1240: 139, 1269: 140, 1284: 141, 1288: 142, 1292: 143, 1325: 144, 1333: 145, 1336: 146, 1340: 147, 1343: 148, 1355: 149, 1358: 150, 1369: 151, 1400: 152, 1402: 153, 1405: 154, 1418: 155, 1420: 156, 1422: 157, 1427: 158, 1430: 159, 1431: 160, 1453: 161, 1458: 162, 1459: 163, 1461: 164, 1464: 165, 1486: 166, 1499: 167, 1512: 168, 1520: 169, 1521: 170, 1522: 171, 1525: 172, 1532: 173, 1541: 174, 1546: 175, 1581: 176, 1583: 177, 1592: 178, 1607: 179, 1613: 180, 1629: 181, 1631: 182, 1649: 183, 1652: 184, 1653: 185, 1659: 186, 1667: 187, 1668: 188, 1705: 189, 1706: 190, 1710: 191, 1715: 192, 1718: 193, 1736: 194, 1765: 195, 1766: 196, 1781: 197, 1786: 198, 1808: 199, 1816: 200, 1818: 201, 1819: 202, 1825: 203, 1832: 204, 1833: 205, 1847: 206, 1852: 207, 1861: 208, 1865: 209, 1866: 210, 1882: 211, 1892: 212, 1917: 213, 1920: 214, 1923: 215, 1927: 216, 1928: 217, 1929: 218, 1945: 219, 1980: 220, 2007: 221, 2008: 222, 2011: 223, 2025: 224, 2041: 225, 2047: 226, 2067: 227}
class4= {23: 0, 27: 1, 49: 2, 72: 3, 123: 4, 134: 5, 142: 6, 147: 7, 152: 8, 167: 9, 179: 10, 180: 11, 200: 12, 224: 13, 245: 14, 259: 15, 264: 16, 275: 17, 277: 18, 286: 19, 288: 20, 294: 21, 301: 22, 310: 23, 314: 24, 329: 25, 362: 26, 379: 27, 384: 28, 423: 29, 449: 30, 506: 31, 539: 32, 548: 33, 567: 34, 569: 35, 578: 36, 584: 37, 593: 38, 599: 39, 617: 40, 618: 41, 622: 42, 649: 43, 666: 44, 668: 45, 697: 46, 717: 47, 728: 48, 748: 49, 755: 50, 757: 51, 778: 52, 783: 53, 798: 54, 808: 55, 813: 56, 818: 57, 831: 58, 838: 59, 894: 60, 913: 61, 963: 62, 989: 63, 991: 64, 998: 65, 1009: 66, 1012: 67, 1020: 68, 1031: 69, 1035: 70, 1038: 71, 1067: 72, 1077: 73, 1078: 74, 1109: 75, 1131: 76, 1139: 77, 1150: 78, 1163: 79, 1181: 80, 1201: 81, 1237: 82, 1241: 83, 1278: 84, 1279: 85, 1281: 86, 1289: 87, 1317: 88, 1337: 89, 1348: 90, 1361: 91, 1379: 92, 1385: 93, 1450: 94, 1451: 95, 1454: 96, 1457: 97, 1466: 98, 1469: 99, 1481: 100, 1501: 101, 1528: 102, 1536: 103, 1545: 104, 1563: 105, 1564: 106, 1596: 107, 1615: 108, 1616: 109, 1625: 110, 1627: 111, 1655: 112, 1669: 113, 1677: 114, 1732: 115, 1757: 116, 1764: 117, 1778: 118, 1784: 119, 1788: 120, 1804: 121, 1826: 122, 1838: 123, 1857: 124, 1875: 125, 1879: 126, 1888: 127, 1904: 128, 1946: 129, 1956: 130, 1965: 131, 1985: 132, 1991: 133, 1996: 134, 1999: 135, 2002: 136, 2010: 137, 2060: 138, 2062: 139}
class5= {1: 0, 8: 1, 15: 2, 17: 3, 21: 4, 35: 5, 38: 6, 47: 7, 50: 8, 51: 9, 66: 10, 79: 11, 104: 12, 108: 13, 110: 14, 112: 15, 128: 16, 132: 17, 137: 18, 155: 19, 163: 20, 172: 21, 186: 22, 190: 23, 198: 24, 206: 25, 213: 26, 222: 27, 237: 28, 247: 29, 273: 30, 276: 31, 278: 32, 279: 33, 291: 34, 292: 35, 309: 36, 312: 37, 315: 38, 345: 39, 374: 40, 375: 41, 382: 42, 392: 43, 408: 44, 427: 45, 436: 46, 441: 47, 444: 48, 445: 49, 466: 50, 471: 51, 485: 52, 488: 53, 490: 54, 493: 55, 503: 56, 511: 57, 519: 58, 526: 59, 527: 60, 536: 61, 547: 62, 553: 63, 554: 64, 555: 65, 568: 66, 586: 67, 590: 68, 591: 69, 592: 70, 613: 71, 631: 72, 671: 73, 674: 74, 680: 75, 685: 76, 691: 77, 698: 78, 701: 79, 708: 80, 712: 81, 718: 82, 720: 83, 721: 84, 727: 85, 729: 86, 746: 87, 747: 88, 752: 89, 770: 90, 772: 91, 777: 92, 779: 93, 780: 94, 791: 95, 806: 96, 807: 97, 814: 98, 819: 99, 820: 100, 824: 101, 837: 102, 839: 103, 844: 104, 863: 105, 870: 106, 871: 107, 874: 108, 880: 109, 881: 110, 891: 111, 898: 112, 900: 113, 905: 114, 909: 115, 925: 116, 930: 117, 932: 118, 935: 119, 938: 120, 943: 121, 957: 122, 960: 123, 965: 124, 967: 125, 974: 126, 975: 127, 977: 128, 980: 129, 986: 130, 995: 131, 1000: 132, 1024: 133, 1027: 134, 1056: 135, 1062: 136, 1081: 137, 1101: 138, 1107: 139, 1113: 140, 1114: 141, 1117: 142, 1122: 143, 1126: 144, 1136: 145, 1146: 146, 1158: 147, 1160: 148, 1164: 149, 1169: 150, 1170: 151, 1176: 152, 1178: 153, 1183: 154, 1194: 155, 1196: 156, 1198: 157, 1200: 158, 1207: 159, 1211: 160, 1213: 161, 1216: 162, 1225: 163, 1228: 164, 1230: 165, 1231: 166, 1238: 167, 1245: 168, 1253: 169, 1261: 170, 1272: 171, 1274: 172, 1277: 173, 1280: 174, 1287: 175, 1291: 176, 1293: 177, 1294: 178, 1302: 179, 1303: 180, 1308: 181, 1311: 182, 1323: 183, 1327: 184, 1335: 185, 1351: 186, 1352: 187, 1362: 188, 1363: 189, 1366: 190, 1367: 191, 1375: 192, 1377: 193, 1384: 194, 1410: 195, 1414: 196, 1416: 197, 1417: 198, 1444: 199, 1446: 200, 1470: 201, 1471: 202, 1484: 203, 1485: 204, 1487: 205, 1489: 206, 1491: 207, 1497: 208, 1504: 209, 1513: 210, 1518: 211, 1535: 212, 1538: 213, 1542: 214, 1556: 215, 1561: 216, 1562: 217, 1569: 218, 1570: 219, 1574: 220, 1575: 221, 1579: 222, 1586: 223, 1598: 224, 1603: 225, 1605: 226, 1610: 227, 1611: 228, 1622: 229, 1624: 230, 1634: 231, 1657: 232, 1664: 233, 1692: 234, 1694: 235, 1697: 236, 1698: 237, 1701: 238, 1711: 239, 1716: 240, 1719: 241, 1723: 242, 1724: 243, 1728: 244, 1739: 245, 1744: 246, 1752: 247, 1771: 248, 1774: 249, 1787: 250, 1796: 251, 1797: 252, 1810: 253, 1815: 254, 1841: 255, 1842: 256, 1846: 257, 1851: 258, 1863: 259, 1868: 260, 1881: 261, 1886: 262, 1901: 263, 1902: 264, 1906: 265, 1912: 266, 1924: 267, 1932: 268, 1949: 269, 1954: 270, 1998: 271, 2021: 272, 2029: 273, 2030: 274, 2034: 275, 2038: 276, 2039: 277, 2048: 278, 2055: 279, 2063: 280}
class6= {7: 0, 25: 1, 46: 2, 48: 3, 76: 4, 94: 5, 100: 6, 145: 7, 154: 8, 156: 9, 214: 10, 215: 11, 223: 12, 285: 13, 295: 14, 381: 15, 409: 16, 459: 17, 465: 18, 706: 19, 846: 20, 873: 21, 912: 22, 997: 23, 1004: 24, 1013: 25, 1073: 26, 1089: 27, 1110: 28, 1118: 29, 1141: 30, 1143: 31, 1202: 32, 1233: 33, 1243: 34, 1256: 35, 1381: 36, 1383: 37, 1406: 38, 1429: 39, 1519: 40, 1527: 41, 1540: 42, 1552: 43, 1582: 44, 1593: 45, 1671: 46, 1681: 47, 1768: 48, 1823: 49, 1855: 50, 1862: 51, 1914: 52, 1939: 53, 1961: 54, 2020: 55, 2026: 56, 2031: 57, 2042: 58, 2051: 59}
class7= {2: 0, 12: 1, 13: 2, 20: 3, 22: 4, 30: 5, 40: 6, 41: 7, 43: 8, 44: 9, 45: 10, 57: 11, 74: 12, 82: 13, 84: 14, 86: 15, 101: 16, 102: 17, 125: 18, 127: 19, 151: 20, 173: 21, 184: 22, 193: 23, 199: 24, 207: 25, 208: 26, 239: 27, 241: 28, 255: 29, 261: 30, 262: 31, 267: 32, 274: 33, 308: 34, 311: 35, 313: 36, 316: 37, 326: 38, 328: 39, 330: 40, 331: 41, 342: 42, 347: 43, 349: 44, 354: 45, 355: 46, 358: 47, 383: 48, 403: 49, 426: 50, 448: 51, 455: 52, 462: 53, 473: 54, 476: 55, 481: 56, 500: 57, 517: 58, 529: 59, 531: 60, 543: 61, 549: 62, 550: 63, 562: 64, 563: 65, 571: 66, 575: 67, 582: 68, 594: 69, 600: 70, 603: 71, 607: 72, 612: 73, 632: 74, 652: 75, 655: 76, 663: 77, 681: 78, 684: 79, 688: 80, 689: 81, 692: 82, 695: 83, 703: 84, 710: 85, 731: 86, 736: 87, 745: 88, 749: 89, 761: 90, 766: 91, 771: 92, 781: 93, 790: 94, 795: 95, 800: 96, 801: 97, 802: 98, 817: 99, 823: 100, 856: 101, 907: 102, 916: 103, 918: 104, 922: 105, 933: 106, 945: 107, 955: 108, 964: 109, 966: 110, 973: 111, 996: 112, 1015: 113, 1018: 114, 1028: 115, 1032: 116, 1046: 117, 1055: 118, 1076: 119, 1079: 120, 1091: 121, 1106: 122, 1116: 123, 1124: 124, 1149: 125, 1152: 126, 1156: 127, 1166: 128, 1171: 129, 1179: 130, 1185: 131, 1199: 132, 1210: 133, 1242: 134, 1246: 135, 1248: 136, 1260: 137, 1267: 138, 1268: 139, 1270: 140, 1271: 141, 1285: 142, 1296: 143, 1298: 144, 1304: 145, 1319: 146, 1321: 147, 1322: 148, 1342: 149, 1349: 150, 1350: 151, 1353: 152, 1374: 153, 1387: 154, 1393: 155, 1394: 156, 1433: 157, 1473: 158, 1488: 159, 1510: 160, 1515: 161, 1526: 162, 1530: 163, 1534: 164, 1566: 165, 1577: 166, 1608: 167, 1633: 168, 1635: 169, 1662: 170, 1663: 171, 1666: 172, 1684: 173, 1685: 174, 1688: 175, 1709: 176, 1725: 177, 1731: 178, 1733: 179, 1737: 180, 1740: 181, 1743: 182, 1746: 183, 1747: 184, 1749: 185, 1754: 186, 1755: 187, 1756: 188, 1767: 189, 1782: 190, 1793: 191, 1794: 192, 1800: 193, 1805: 194, 1813: 195, 1821: 196, 1827: 197, 1830: 198, 1839: 199, 1848: 200, 1854: 201, 1856: 202, 1869: 203, 1877: 204, 1899: 205, 1905: 206, 1908: 207, 1909: 208, 1913: 209, 1916: 210, 1918: 211, 1926: 212, 1937: 213, 1940: 214, 1942: 215, 1943: 216, 1944: 217, 1952: 218, 1960: 219, 1962: 220, 1974: 221, 1975: 222, 1979: 223, 1987: 224, 1995: 225, 1997: 226, 2000: 227, 2003: 228, 2006: 229, 2017: 230, 2023: 231, 2028: 232, 2043: 233, 2046: 234, 2064: 235}
class8= {0: 0, 4: 1, 6: 2, 11: 3, 16: 4, 19: 5, 32: 6, 52: 7, 55: 8, 67: 9, 69: 10, 70: 11, 73: 12, 78: 13, 85: 14, 89: 15, 95: 16, 107: 17, 118: 18, 122: 19, 124: 20, 140: 21, 141: 22, 148: 23, 149: 24, 150: 25, 170: 26, 181: 27, 182: 28, 202: 29, 203: 30, 205: 31, 228: 32, 232: 33, 253: 34, 270: 35, 287: 36, 305: 37, 340: 38, 343: 39, 348: 40, 357: 41, 368: 42, 385: 43, 391: 44, 398: 45, 400: 46, 411: 47, 424: 48, 437: 49, 452: 50, 491: 51, 504: 52, 505: 53, 508: 54, 523: 55, 534: 56, 542: 57, 545: 58, 552: 59, 598: 60, 601: 61, 602: 62, 626: 63, 627: 64, 629: 65, 677: 66, 693: 67, 702: 68, 725: 69, 726: 70, 732: 71, 733: 72, 742: 73, 754: 74, 756: 75, 759: 76, 765: 77, 803: 78, 829: 79, 840: 80, 855: 81, 864: 82, 893: 83, 895: 84, 897: 85, 901: 86, 910: 87, 934: 88, 941: 89, 950: 90, 951: 91, 956: 92, 972: 93, 976: 94, 984: 95, 987: 96, 988: 97, 992: 98, 993: 99, 999: 100, 1008: 101, 1021: 102, 1026: 103, 1033: 104, 1039: 105, 1063: 106, 1065: 107, 1074: 108, 1080: 109, 1086: 110, 1087: 111, 1093: 112, 1098: 113, 1105: 114, 1123: 115, 1134: 116, 1142: 117, 1148: 118, 1172: 119, 1182: 120, 1184: 121, 1209: 122, 1214: 123, 1223: 124, 1227: 125, 1252: 126, 1255: 127, 1259: 128, 1263: 129, 1290: 130, 1300: 131, 1306: 132, 1312: 133, 1326: 134, 1328: 135, 1364: 136, 1391: 137, 1397: 138, 1403: 139, 1415: 140, 1419: 141, 1435: 142, 1437: 143, 1440: 144, 1441: 145, 1442: 146, 1449: 147, 1452: 148, 1468: 149, 1475: 150, 1502: 151, 1509: 152, 1511: 153, 1529: 154, 1537: 155, 1539: 156, 1557: 157, 1567: 158, 1584: 159, 1600: 160, 1602: 161, 1604: 162, 1617: 163, 1646: 164, 1647: 165, 1648: 166, 1665: 167, 1674: 168, 1700: 169, 1712: 170, 1721: 171, 1726: 172, 1727: 173, 1734: 174, 1741: 175, 1753: 176, 1758: 177, 1761: 178, 1775: 179, 1779: 180, 1780: 181, 1789: 182, 1790: 183, 1798: 184, 1801: 185, 1802: 186, 1803: 187, 1817: 188, 1820: 189, 1867: 190, 1894: 191, 1895: 192, 1907: 193, 1919: 194, 1936: 195, 1951: 196, 1957: 197, 1958: 198, 1966: 199, 1970: 200, 1992: 201, 2014: 202, 2018: 203, 2052: 204, 2059: 205}
class9= {60: 0, 106: 1, 166: 2, 290: 3, 298: 4, 388: 5, 438: 6, 480: 7, 587: 8, 796: 9, 832: 10, 899: 11, 1059: 12, 1128: 13, 1140: 14, 1365: 15, 1378: 16, 1428: 17, 1434: 18, 1507: 19, 1811: 20, 1874: 21, 2040: 22}

class0_reverse = {value: i for i, value in class0.items()}
class1_reverse = {value: i for i, value in class1.items()}
class2_reverse = {value: i for i, value in class2.items()}
class3_reverse = {value: i for i, value in class3.items()}
class4_reverse = {value: i for i, value in class4.items()}
class5_reverse = {value: i for i, value in class5.items()}
class6_reverse = {value: i for i, value in class6.items()}
class7_reverse = {value: i for i, value in class7.items()}
class8_reverse = {value: i for i, value in class8.items()}
class9_reverse = {value: i for i, value in class9.items()}

# 定义读取文件的格式
def default_loader(path):
    #return Image.open(path).convert('L')
    img = cv2.imread(path, -1)
    return img
def npy_loader(path):
    npy = np.load(path)
    return npy
# 首先继承上面的dataset类。然后在__init__()方法中得到图像的路径，然后将图像路径组成一个数组，这样在__getitim__()中就可以直接读取：
class MyDataset(Dataset):  # 创建自己的类：MyDataset,这个类是继承的torch.utils.data.Dataset
    def __init__(self, root, txt, transform=None, target_transform=None, loader=default_loader):  # 初始化一些需要传入的参数
        super(MyDataset, self).__init__()  # 对继承自父类的属性进行初始化
        fh = open(txt, 'r')  # 按照传入的路径和txt文本参数，打开这个文本，并读取内容
        imgs = []
        #data=[]
        for line in fh:  # 迭代该列表#按行循环txt文本中的内
            line = line.strip('\n')
            line = line.rstrip('\n')  # 删除 本行string 字符串末尾的指定字符，这个方法的详细介绍自己查询python
            words = line.split()  # 用split将该行分割成列表  split的默认参数是空格，所以不传递任何参数时分割空格
            imgs.append((words[0], int(words[1])))  # 把txt里的内容读入imgs列表保存，具体是words几要看txt内容而定
            # 很显然，根据我刚才截图所示txt的内容，words[0]是图片信息，words[1]是lable

        A = np.load(root+'/'+bmname+'_w1.npy')
        A = torch.tensor(A, dtype=torch.float)
        self.A = A
        self.imgs = imgs
        self.data = Data
        self.transform = transform
        self.target_transform = target_transform
        self.loader = loader

    def __getitem__(self, index):
        fx, label = self.imgs[index]  # fn是图片path #fn和label分别获得imgs[index]也即是刚才每行中word[0]和word[1]的信息
        x = self.loader(fx)
        x = torch.tensor(x, dtype=torch.long)
        if x.dim() == 1:
            node_labels = x.unsqueeze(-1)
        node_labels = node_labels - node_labels.min(dim=0)[0]
        node_labels = node_labels.unbind(dim=-1)
        node_labels = [F.one_hot(x, num_classes=-1) for x in node_labels]
        node_labels = torch.cat(node_labels, dim=-1).to(torch.float)
        # img = self.loader(fn)  # 按照路径读取图片
        # img = torch.tensor(img,dtype=torch.float)
        label = torch.tensor(label,dtype=torch.long)
        label = label.unsqueeze(0)
        # if self.transform is not None:
        #     img = self.transform(img)  # 数据标签转换为Tensor
        # coo_A = coo_matrix(img)  # 邻接矩阵的边的行/列的坐标
        # edge_index = [coo_A.row, coo_A.col]
        # edge_index = torch.tensor(edge_index,dtype=torch.long)
        ones = np.ones(len(x))
        mask = np.array(ones).astype("bool")
        mask = torch.tensor(mask, dtype=bool)
        data = Data(x=node_labels, adj=self.A, y=label, mask=mask)
        return data

    def __len__(self):
        return len(self.imgs)
def collate_fn(data_list: List[Data]) -> Batch:
    batch = Batch()
    for key in data_list[0].keys:
        batch[key] = default_collate([data[key] for data in data_list])
    return batch

max_nodes = 2068
bmname = 'p2p2068_p0.1_m50_c7_train'
root = '/home/iot/zcy/usb/copy/myGCN/cnn_data/'+bmname
train_dataset = MyDataset(root, txt=root+'/train.txt', transform=None, loader=npy_loader)
num_features = train_dataset.num_features
val_dataset = MyDataset(root, txt=root+'/test.txt', transform=None, loader=npy_loader)
train_loader = DataLoader(dataset=train_dataset, batch_size=10, shuffle=True, num_workers=1,collate_fn=collate_fn)
val_loader = DataLoader(dataset=val_dataset, batch_size=10, shuffle=True, num_workers=1,collate_fn=collate_fn)
#test_dataloader = DataLoader(dataset=test_dataset, batch_size=20, shuffle=True, num_workers=1)
print('num_of_trainData:', len(train_dataset))
print('num_of_valData:', len(val_dataset))

class GNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels,
                 normalize=False, lin=True):
        super().__init__()

        self.conv1 = DenseSAGEConv(in_channels, hidden_channels, normalize)
        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)
        self.conv2 = DenseSAGEConv(hidden_channels, hidden_channels, normalize)
        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)
        self.conv3 = DenseSAGEConv(hidden_channels, out_channels, normalize)
        self.bn3 = torch.nn.BatchNorm1d(out_channels)

        if lin is True:
            self.lin = torch.nn.Linear(2 * hidden_channels + out_channels,
                                       out_channels)
        else:
            self.lin = None

    def bn(self, i, x):
        batch_size, num_nodes, num_channels = x.size()

        x = x.view(-1, num_channels)
        x = getattr(self, f'bn{i}')(x)
        x = x.view(batch_size, num_nodes, num_channels)
        return x

    def forward(self, x, adj, mask=None):
        batch_size, num_nodes, in_channels = x.size()

        x0 = x
        x1 = self.bn(1, F.relu(self.conv1(x0, adj, mask)))
        x2 = self.bn(2, F.relu(self.conv2(x1, adj, mask)))
        x3 = self.bn(3, F.relu(self.conv3(x2, adj, mask)))

        x = torch.cat([x1, x2, x3], dim=-1)

        if self.lin is not None:
            x = F.relu(self.lin(x))

        return x


class Net(torch.nn.Module):
    def __init__(self):
        super().__init__()

        num_nodes = ceil(0.25 * max_nodes)
        self.gnn1_pool = GNN(num_features, 64, num_nodes)
        self.gnn1_embed = GNN(num_features, 64, 64, lin=False)

        # num_nodes = ceil(0.25 * num_nodes)
        # self.gnn2_pool = GNN(3 * 64, 64, num_nodes)
        # self.gnn2_embed = GNN(3 * 64, 64, 64, lin=False)

        self.gnn3_embed = GNN(3 * 64, 64, 64, lin=False)

        self.lin1 = torch.nn.Linear(3 * 64, 64)
        self.lin2 = torch.nn.Linear(64, 236)    ## output num_classes

    def forward(self, x, adj, mask=None):
        s = self.gnn1_pool(x, adj, mask)
        x = self.gnn1_embed(x, adj, mask)

        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)

        # s = self.gnn2_pool(x, adj)
        # x = self.gnn2_embed(x, adj)
        #
        # x, adj, l2, e2 = dense_diff_pool(x, adj, s)

        x = self.gnn3_embed(x, adj)

        x = x.mean(dim=1)
        x = F.relu(self.lin1(x))
        x = self.lin2(x)
        #return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2
        return F.log_softmax(x, dim=-1), l1, e1

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Net().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


def train(epoch):
    model.train()
    loss_all = 0

    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        output, _, _ = model(data.x, data.adj, data.mask)
        loss = F.nll_loss(output, data.y.view(-1))
        loss.backward()
        loss_all += data.y.size(0) * loss.item()
        optimizer.step()
    return loss_all / len(train_dataset)


# @torch.no_grad()
# def test(loader):
#     model.eval()
#     correct = 0
#
#     for data in loader:
#         data = data.to(device)
#         pred = model(data.x, data.adj, data.mask)[0].max(dim=1)[1]
#         correct += pred.eq(data.y.view(-1)).sum().item()
#     return correct / len(loader.dataset)
@torch.no_grad()                 #加上GCN distance
def test(loader):
    model.eval()
    correct = 0
    labels = []
    preds = []
    for data in loader:
        data = data.to(device)
        pred = model(data.x, data.adj, data.mask)[0].max(dim=1)[1]
        correct += pred.eq(data.y.view(-1)).sum().item()

        labels.append(np.squeeze(data['y'].data.numpy()))
        preds.append(pred.cpu().data.numpy())

    labels = np.hstack(labels)
    preds = np.hstack(preds)
    # print("labels:", labels)
    # print("preds:", preds)
    real_labels = []
    for n in labels:
        real_labels.append(class7_reverse[n])
    real_preds = []
    for m in preds:
        real_preds.append(class7_reverse[m])
    print('reallabels:', real_labels)
    print('realpreds:', real_preds)
    read_dic = np.load("p2p2068"+"_short_path.npy", allow_pickle=True).item()
    # print(read_dic[2][3])
    distance = []
    for i in range(len(labels)):
        a = read_dic[real_labels[i]][real_preds[i]]
        distance.append(a)
    #print(distance)
    result_dis = {}
    sum_dis = 0
    for i in set(distance):
        result_dis[i] = distance.count(i)
        sum_dis = i * result_dis[i] + sum_dis
    average_dis = sum_dis / len(labels)

    return correct / len(loader.dataset), average_dis

best_val_acc = test_acc = 0
for epoch in range(1, 101):
    train_loss = train(epoch)
    val_acc, val_GCN = test(val_loader)
    # if val_acc > best_val_acc:
    #     test_acc, test_GCN = test(test_loader)
    #     best_val_acc = val_acc
    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '
          f'Val Acc: {val_acc:.4f} ,Val_GCN: {val_GCN:.4f}')
    if epoch % 10 == 0:
        torch.save(model.state_dict(), 'model/' + bmname + '_p1.pt')
#保存模型
torch.save(model.state_dict(), 'model/' + bmname + '_p1.pt')
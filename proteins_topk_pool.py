import os.path as osp

import torch
import torch.nn.functional as F

from torch_geometric.datasets import TUDataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GraphConv, TopKPooling
from torch_geometric.nn import global_max_pool as gmp
from torch_geometric.nn import global_mean_pool as gap
from MyData_fromTU import MyData_fromTU
import numpy as np

# path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'PROTEINS')
# dataset = TUDataset(path, name='PROTEINS')
bmname = 'dolphins'
sname = '_p0.1_m50'
path = osp.join(osp.dirname(osp.realpath(__file__)), 'data', bmname)
dataset = MyData_fromTU(path, name=bmname+sname).shuffle()
dataset = dataset.shuffle()
n = len(dataset) // 10
test_dataset = dataset[:n]
train_dataset = dataset[n:]
test_loader = DataLoader(test_dataset, batch_size=60)
train_loader = DataLoader(train_dataset, batch_size=60)

class0= {10: 0, 18: 1, 24: 2, 26: 3, 36: 4, 41: 5, 46: 6, 59: 7, 61: 8, 63: 9, 88: 10, 97: 11, 115: 12, 120: 13, 121: 14, 129: 15, 130: 16, 131: 17, 153: 18, 157: 19, 162: 20, 164: 21, 178: 22, 183: 23, 187: 24, 195: 25, 210: 26, 211: 27, 212: 28, 214: 29, 220: 30, 225: 31, 236: 32, 258: 33, 266: 34, 271: 35, 281: 36, 284: 37, 293: 38, 303: 39, 304: 40, 325: 41, 327: 42, 332: 43, 334: 44, 336: 45, 359: 46, 366: 47, 370: 48, 380: 49, 384: 50, 393: 51, 394: 52, 399: 53, 413: 54, 428: 55, 430: 56, 432: 57, 450: 58, 454: 59, 457: 60, 474: 61, 492: 62, 499: 63, 518: 64, 533: 65, 546: 66, 558: 67, 565: 68, 570: 69, 572: 70, 576: 71, 579: 72, 583: 73, 614: 74, 616: 75, 619: 76, 635: 77, 636: 78, 639: 79, 643: 80, 650: 81, 651: 82, 657: 83, 658: 84, 659: 85, 665: 86, 667: 87, 672: 88, 673: 89, 682: 90, 710: 91, 716: 92, 719: 93, 728: 94, 739: 95, 740: 96, 744: 97, 758: 98, 762: 99, 764: 100, 773: 101, 785: 102, 787: 103, 788: 104, 795: 105, 797: 106, 812: 107, 826: 108, 831: 109, 835: 110, 836: 111, 850: 112, 852: 113, 854: 114, 860: 115, 872: 116, 879: 117, 882: 118, 890: 119, 902: 120, 903: 121, 904: 122, 906: 123, 923: 124, 947: 125, 954: 126, 958: 127, 971: 128, 981: 129, 982: 130, 989: 131, 1014: 132, 1016: 133, 1022: 134, 1036: 135, 1040: 136, 1042: 137, 1051: 138, 1057: 139, 1064: 140, 1068: 141, 1072: 142, 1083: 143, 1085: 144, 1094: 145, 1096: 146, 1108: 147, 1111: 148, 1120: 149, 1141: 150, 1144: 151, 1162: 152, 1173: 153, 1186: 154, 1189: 155, 1190: 156, 1210: 157, 1217: 158, 1234: 159, 1236: 160, 1240: 161, 1258: 162, 1269: 163, 1284: 164, 1285: 165, 1288: 166, 1292: 167, 1298: 168, 1304: 169, 1317: 170, 1325: 171, 1333: 172, 1336: 173, 1340: 174, 1343: 175, 1353: 176, 1355: 177, 1358: 178, 1369: 179, 1400: 180, 1402: 181, 1405: 182, 1418: 183, 1420: 184, 1422: 185, 1427: 186, 1430: 187, 1431: 188, 1453: 189, 1458: 190, 1459: 191, 1461: 192, 1464: 193, 1481: 194, 1482: 195, 1486: 196, 1499: 197, 1501: 198, 1512: 199, 1521: 200, 1522: 201, 1525: 202, 1532: 203, 1541: 204, 1546: 205, 1581: 206, 1583: 207, 1592: 208, 1607: 209, 1613: 210, 1616: 211, 1629: 212, 1631: 213, 1649: 214, 1652: 215, 1653: 216, 1659: 217, 1667: 218, 1668: 219, 1681: 220, 1705: 221, 1706: 222, 1710: 223, 1715: 224, 1718: 225, 1736: 226, 1743: 227, 1754: 228, 1765: 229, 1766: 230, 1781: 231, 1786: 232, 1808: 233, 1812: 234, 1816: 235, 1818: 236, 1819: 237, 1825: 238, 1832: 239, 1833: 240, 1847: 241, 1852: 242, 1861: 243, 1865: 244, 1866: 245, 1872: 246, 1882: 247, 1892: 248, 1913: 249, 1917: 250, 1920: 251, 1923: 252, 1927: 253, 1928: 254, 1929: 255, 1945: 256, 1959: 257, 1979: 258, 1980: 259, 2007: 260, 2008: 261, 2011: 262, 2041: 263, 2046: 264, 2047: 265, 2058: 266, 2062: 267, 2067: 268}
class1= {1: 0, 2: 1, 6: 2, 7: 3, 8: 4, 15: 5, 17: 6, 21: 7, 25: 8, 29: 9, 33: 10, 34: 11, 35: 12, 38: 13, 43: 14, 45: 15, 47: 16, 50: 17, 51: 18, 54: 19, 55: 20, 58: 21, 65: 22, 66: 23, 71: 24, 74: 25, 79: 26, 83: 27, 91: 28, 94: 29, 99: 30, 100: 31, 102: 32, 104: 33, 106: 34, 108: 35, 110: 36, 112: 37, 113: 38, 114: 39, 116: 40, 126: 41, 128: 42, 132: 43, 136: 44, 137: 45, 145: 46, 149: 47, 155: 48, 156: 49, 159: 50, 163: 51, 168: 52, 169: 53, 172: 54, 175: 55, 186: 56, 190: 57, 197: 58, 198: 59, 204: 60, 206: 61, 208: 62, 213: 63, 215: 64, 219: 65, 222: 66, 229: 67, 233: 68, 235: 69, 237: 70, 238: 71, 239: 72, 241: 73, 247: 74, 262: 75, 263: 76, 273: 77, 276: 78, 278: 79, 291: 80, 292: 81, 295: 82, 298: 83, 309: 84, 312: 85, 315: 86, 321: 87, 322: 88, 326: 89, 335: 90, 342: 91, 345: 92, 346: 93, 349: 94, 363: 95, 369: 96, 372: 97, 374: 98, 375: 99, 377: 100, 381: 101, 382: 102, 387: 103, 392: 104, 397: 105, 403: 106, 408: 107, 409: 108, 410: 109, 414: 110, 427: 111, 433: 112, 435: 113, 436: 114, 441: 115, 444: 116, 445: 117, 447: 118, 448: 119, 465: 120, 466: 121, 467: 122, 471: 123, 485: 124, 487: 125, 488: 126, 490: 127, 493: 128, 496: 129, 500: 130, 505: 131, 511: 132, 513: 133, 516: 134, 519: 135, 526: 136, 527: 137, 531: 138, 536: 139, 547: 140, 553: 141, 554: 142, 555: 143, 556: 144, 563: 145, 568: 146, 574: 147, 586: 148, 590: 149, 591: 150, 592: 151, 594: 152, 597: 153, 608: 154, 610: 155, 612: 156, 613: 157, 623: 158, 628: 159, 631: 160, 652: 161, 654: 162, 662: 163, 664: 164, 669: 165, 671: 166, 674: 167, 676: 168, 678: 169, 679: 170, 680: 171, 685: 172, 689: 173, 690: 174, 691: 175, 694: 176, 698: 177, 699: 178, 701: 179, 705: 180, 706: 181, 708: 182, 712: 183, 718: 184, 720: 185, 721: 186, 722: 187, 727: 188, 729: 189, 736: 190, 745: 191, 746: 192, 747: 193, 750: 194, 752: 195, 753: 196, 760: 197, 761: 198, 766: 199, 769: 200, 770: 201, 772: 202, 775: 203, 777: 204, 779: 205, 780: 206, 786: 207, 791: 208, 800: 209, 804: 210, 806: 211, 807: 212, 814: 213, 819: 214, 820: 215, 822: 216, 824: 217, 825: 218, 837: 219, 839: 220, 841: 221, 844: 222, 845: 223, 846: 224, 847: 225, 851: 226, 856: 227, 863: 228, 870: 229, 871: 230, 873: 231, 874: 232, 880: 233, 881: 234, 891: 235, 898: 236, 899: 237, 900: 238, 905: 239, 912: 240, 914: 241, 915: 242, 920: 243, 921: 244, 925: 245, 930: 246, 932: 247, 935: 248, 938: 249, 939: 250, 942: 251, 949: 252, 957: 253, 960: 254, 965: 255, 967: 256, 968: 257, 973: 258, 974: 259, 975: 260, 980: 261, 986: 262, 987: 263, 995: 264, 996: 265, 997: 266, 1000: 267, 1002: 268, 1010: 269, 1013: 270, 1015: 271, 1017: 272, 1024: 273, 1026: 274, 1027: 275, 1030: 276, 1056: 277, 1062: 278, 1063: 279, 1075: 280, 1076: 281, 1081: 282, 1101: 283, 1107: 284, 1110: 285, 1112: 286, 1113: 287, 1114: 288, 1117: 289, 1119: 290, 1122: 291, 1124: 292, 1126: 293, 1136: 294, 1140: 295, 1143: 296, 1146: 297, 1147: 298, 1148: 299, 1158: 300, 1160: 301, 1161: 302, 1164: 303, 1169: 304, 1170: 305, 1171: 306, 1175: 307, 1176: 308, 1178: 309, 1183: 310, 1194: 311, 1195: 312, 1196: 313, 1197: 314, 1198: 315, 1200: 316, 1207: 317, 1211: 318, 1212: 319, 1213: 320, 1216: 321, 1218: 322, 1225: 323, 1228: 324, 1230: 325, 1231: 326, 1238: 327, 1242: 328, 1245: 329, 1247: 330, 1248: 331, 1253: 332, 1259: 333, 1261: 334, 1272: 335, 1274: 336, 1277: 337, 1280: 338, 1287: 339, 1291: 340, 1293: 341, 1294: 342, 1302: 343, 1303: 344, 1307: 345, 1308: 346, 1311: 347, 1323: 348, 1327: 349, 1329: 350, 1335: 351, 1342: 352, 1347: 353, 1351: 354, 1352: 355, 1356: 356, 1357: 357, 1362: 358, 1363: 359, 1366: 360, 1367: 361, 1374: 362, 1375: 363, 1377: 364, 1378: 365, 1381: 366, 1383: 367, 1384: 368, 1393: 369, 1401: 370, 1410: 371, 1414: 372, 1416: 373, 1417: 374, 1433: 375, 1436: 376, 1438: 377, 1442: 378, 1444: 379, 1446: 380, 1467: 381, 1470: 382, 1471: 383, 1473: 384, 1478: 385, 1484: 386, 1485: 387, 1487: 388, 1489: 389, 1491: 390, 1492: 391, 1497: 392, 1500: 393, 1504: 394, 1505: 395, 1506: 396, 1513: 397, 1515: 398, 1516: 399, 1518: 400, 1530: 401, 1535: 402, 1538: 403, 1539: 404, 1542: 405, 1544: 406, 1548: 407, 1552: 408, 1556: 409, 1558: 410, 1561: 411, 1562: 412, 1569: 413, 1570: 414, 1574: 415, 1575: 416, 1579: 417, 1582: 418, 1586: 419, 1588: 420, 1598: 421, 1602: 422, 1603: 423, 1605: 424, 1608: 425, 1610: 426, 1611: 427, 1622: 428, 1624: 429, 1634: 430, 1635: 431, 1638: 432, 1641: 433, 1642: 434, 1644: 435, 1657: 436, 1660: 437, 1663: 438, 1664: 439, 1671: 440, 1672: 441, 1673: 442, 1675: 443, 1684: 444, 1685: 445, 1691: 446, 1692: 447, 1694: 448, 1697: 449, 1698: 450, 1701: 451, 1711: 452, 1714: 453, 1716: 454, 1719: 455, 1723: 456, 1724: 457, 1728: 458, 1731: 459, 1733: 460, 1738: 461, 1739: 462, 1742: 463, 1744: 464, 1749: 465, 1752: 466, 1767: 467, 1771: 468, 1774: 469, 1787: 470, 1789: 471, 1795: 472, 1796: 473, 1797: 474, 1806: 475, 1810: 476, 1813: 477, 1815: 478, 1823: 479, 1824: 480, 1834: 481, 1841: 482, 1842: 483, 1846: 484, 1848: 485, 1849: 486, 1850: 487, 1851: 488, 1856: 489, 1863: 490, 1868: 491, 1881: 492, 1884: 493, 1885: 494, 1886: 495, 1897: 496, 1901: 497, 1902: 498, 1906: 499, 1907: 500, 1912: 501, 1914: 502, 1915: 503, 1924: 504, 1932: 505, 1936: 506, 1939: 507, 1940: 508, 1944: 509, 1949: 510, 1953: 511, 1954: 512, 1967: 513, 1987: 514, 1998: 515, 2021: 516, 2024: 517, 2029: 518, 2030: 519, 2034: 520, 2035: 521, 2036: 522, 2038: 523, 2039: 524, 2042: 525, 2045: 526, 2048: 527, 2049: 528, 2051: 529, 2055: 530, 2063: 531}
class2= {0: 0, 4: 1, 5: 2, 19: 3, 42: 4, 60: 5, 62: 6, 64: 7, 69: 8, 73: 9, 80: 10, 85: 11, 89: 12, 111: 13, 118: 14, 119: 15, 122: 16, 124: 17, 140: 18, 141: 19, 143: 20, 148: 21, 150: 22, 161: 23, 170: 24, 171: 25, 174: 26, 176: 27, 181: 28, 182: 29, 191: 30, 196: 31, 202: 32, 203: 33, 205: 34, 231: 35, 232: 36, 243: 37, 260: 38, 265: 39, 270: 40, 279: 41, 280: 42, 285: 43, 287: 44, 290: 45, 297: 46, 302: 47, 305: 48, 317: 49, 318: 50, 319: 51, 343: 52, 348: 53, 357: 54, 360: 55, 373: 56, 376: 57, 378: 58, 386: 59, 388: 60, 391: 61, 398: 62, 406: 63, 407: 64, 411: 65, 416: 66, 418: 67, 422: 68, 431: 69, 438: 70, 452: 71, 460: 72, 463: 73, 468: 74, 472: 75, 478: 76, 480: 77, 482: 78, 486: 79, 489: 80, 491: 81, 503: 82, 504: 83, 508: 84, 510: 85, 514: 86, 523: 87, 534: 88, 541: 89, 542: 90, 545: 91, 552: 92, 559: 93, 573: 94, 577: 95, 585: 96, 587: 97, 588: 98, 589: 99, 598: 100, 601: 101, 602: 102, 604: 103, 606: 104, 609: 105, 626: 106, 627: 107, 633: 108, 640: 109, 641: 110, 642: 111, 647: 112, 648: 113, 670: 114, 677: 115, 693: 116, 709: 117, 713: 118, 715: 119, 725: 120, 726: 121, 730: 122, 732: 123, 735: 124, 737: 125, 742: 126, 765: 127, 767: 128, 789: 129, 796: 130, 803: 131, 809: 132, 811: 133, 816: 134, 828: 135, 829: 136, 832: 137, 833: 138, 840: 139, 842: 140, 848: 141, 849: 142, 855: 143, 864: 144, 866: 145, 868: 146, 869: 147, 878: 148, 883: 149, 889: 150, 893: 151, 897: 152, 901: 153, 909: 154, 910: 155, 911: 156, 917: 157, 928: 158, 934: 159, 936: 160, 937: 161, 941: 162, 943: 163, 944: 164, 956: 165, 959: 166, 962: 167, 969: 168, 972: 169, 977: 170, 984: 171, 990: 172, 992: 173, 993: 174, 1005: 175, 1006: 176, 1008: 177, 1011: 178, 1019: 179, 1021: 180, 1023: 181, 1033: 182, 1041: 183, 1049: 184, 1053: 185, 1059: 186, 1060: 187, 1065: 188, 1071: 189, 1074: 190, 1084: 191, 1086: 192, 1090: 193, 1095: 194, 1098: 195, 1100: 196, 1115: 197, 1123: 198, 1128: 199, 1132: 200, 1133: 201, 1134: 202, 1142: 203, 1157: 204, 1159: 205, 1172: 206, 1182: 207, 1184: 208, 1188: 209, 1192: 210, 1193: 211, 1209: 212, 1214: 213, 1219: 214, 1221: 215, 1223: 216, 1227: 217, 1232: 218, 1239: 219, 1243: 220, 1249: 221, 1250: 222, 1252: 223, 1254: 224, 1255: 225, 1257: 226, 1262: 227, 1263: 228, 1265: 229, 1276: 230, 1283: 231, 1290: 232, 1295: 233, 1309: 234, 1312: 235, 1313: 236, 1314: 237, 1315: 238, 1326: 239, 1328: 240, 1332: 241, 1334: 242, 1338: 243, 1345: 244, 1346: 245, 1365: 246, 1370: 247, 1380: 248, 1391: 249, 1396: 250, 1398: 251, 1403: 252, 1409: 253, 1415: 254, 1419: 255, 1425: 256, 1428: 257, 1434: 258, 1435: 259, 1437: 260, 1441: 261, 1443: 262, 1449: 263, 1460: 264, 1468: 265, 1475: 266, 1479: 267, 1483: 268, 1494: 269, 1502: 270, 1503: 271, 1507: 272, 1509: 273, 1511: 274, 1514: 275, 1529: 276, 1533: 277, 1547: 278, 1550: 279, 1555: 280, 1557: 281, 1559: 282, 1567: 283, 1568: 284, 1573: 285, 1578: 286, 1580: 287, 1584: 288, 1594: 289, 1600: 290, 1601: 291, 1604: 292, 1617: 293, 1623: 294, 1637: 295, 1639: 296, 1645: 297, 1647: 298, 1651: 299, 1674: 300, 1689: 301, 1690: 302, 1703: 303, 1704: 304, 1712: 305, 1717: 306, 1722: 307, 1726: 308, 1734: 309, 1741: 310, 1750: 311, 1760: 312, 1761: 313, 1762: 314, 1768: 315, 1772: 316, 1775: 317, 1777: 318, 1783: 319, 1785: 320, 1790: 321, 1792: 322, 1799: 323, 1801: 324, 1802: 325, 1803: 326, 1811: 327, 1817: 328, 1820: 329, 1822: 330, 1829: 331, 1836: 332, 1843: 333, 1870: 334, 1871: 335, 1874: 336, 1878: 337, 1883: 338, 1894: 339, 1895: 340, 1896: 341, 1930: 342, 1948: 343, 1951: 344, 1957: 345, 1958: 346, 1966: 347, 1970: 348, 1971: 349, 1972: 350, 1978: 351, 1982: 352, 1986: 353, 1988: 354, 1990: 355, 1992: 356, 2001: 357, 2005: 358, 2014: 359, 2015: 360, 2018: 361, 2019: 362, 2022: 363, 2032: 364, 2037: 365, 2040: 366, 2059: 367, 2066: 368}
class3= {13: 0, 20: 1, 22: 2, 23: 3, 27: 4, 40: 5, 49: 6, 57: 7, 72: 8, 84: 9, 86: 10, 101: 11, 123: 12, 125: 13, 127: 14, 134: 15, 142: 16, 147: 17, 151: 18, 152: 19, 167: 20, 173: 21, 179: 22, 180: 23, 184: 24, 199: 25, 200: 26, 224: 27, 245: 28, 255: 29, 259: 30, 261: 31, 264: 32, 274: 33, 275: 34, 277: 35, 286: 36, 288: 37, 294: 38, 301: 39, 308: 40, 310: 41, 311: 42, 314: 43, 316: 44, 329: 45, 330: 46, 331: 47, 347: 48, 354: 49, 355: 50, 358: 51, 362: 52, 379: 53, 383: 54, 423: 55, 426: 56, 449: 57, 455: 58, 462: 59, 473: 60, 476: 61, 481: 62, 506: 63, 517: 64, 539: 65, 543: 66, 548: 67, 549: 68, 567: 69, 569: 70, 571: 71, 578: 72, 584: 73, 593: 74, 599: 75, 600: 76, 617: 77, 618: 78, 622: 79, 632: 80, 649: 81, 655: 82, 663: 83, 666: 84, 668: 85, 681: 86, 688: 87, 697: 88, 703: 89, 717: 90, 731: 91, 748: 92, 749: 93, 751: 94, 755: 95, 757: 96, 771: 97, 778: 98, 783: 99, 790: 100, 798: 101, 802: 102, 808: 103, 813: 104, 817: 105, 818: 106, 838: 107, 894: 108, 907: 109, 913: 110, 916: 111, 918: 112, 922: 113, 933: 114, 945: 115, 955: 116, 963: 117, 964: 118, 966: 119, 991: 120, 998: 121, 1009: 122, 1012: 123, 1018: 124, 1020: 125, 1031: 126, 1032: 127, 1035: 128, 1038: 129, 1046: 130, 1055: 131, 1067: 132, 1077: 133, 1078: 134, 1079: 135, 1091: 136, 1106: 137, 1109: 138, 1116: 139, 1131: 140, 1139: 141, 1150: 142, 1152: 143, 1163: 144, 1166: 145, 1181: 146, 1185: 147, 1201: 148, 1237: 149, 1241: 150, 1246: 151, 1267: 152, 1268: 153, 1270: 154, 1271: 155, 1278: 156, 1279: 157, 1281: 158, 1289: 159, 1296: 160, 1319: 161, 1322: 162, 1337: 163, 1348: 164, 1349: 165, 1350: 166, 1361: 167, 1379: 168, 1385: 169, 1387: 170, 1450: 171, 1451: 172, 1454: 173, 1457: 174, 1466: 175, 1469: 176, 1488: 177, 1510: 178, 1520: 179, 1526: 180, 1528: 181, 1534: 182, 1536: 183, 1545: 184, 1563: 185, 1564: 186, 1566: 187, 1577: 188, 1596: 189, 1615: 190, 1625: 191, 1627: 192, 1633: 193, 1655: 194, 1666: 195, 1669: 196, 1677: 197, 1688: 198, 1709: 199, 1732: 200, 1737: 201, 1740: 202, 1746: 203, 1755: 204, 1757: 205, 1764: 206, 1778: 207, 1782: 208, 1784: 209, 1788: 210, 1793: 211, 1800: 212, 1804: 213, 1821: 214, 1826: 215, 1830: 216, 1838: 217, 1839: 218, 1854: 219, 1857: 220, 1869: 221, 1875: 222, 1877: 223, 1879: 224, 1888: 225, 1904: 226, 1909: 227, 1918: 228, 1937: 229, 1942: 230, 1943: 231, 1946: 232, 1956: 233, 1962: 234, 1965: 235, 1974: 236, 1975: 237, 1985: 238, 1991: 239, 1996: 240, 1999: 241, 2000: 242, 2002: 243, 2003: 244, 2010: 245, 2017: 246, 2023: 247, 2025: 248, 2028: 249, 2043: 250, 2060: 251, 2064: 252}
class4= {3: 0, 9: 1, 11: 2, 12: 3, 14: 4, 16: 5, 28: 6, 30: 7, 31: 8, 32: 9, 37: 10, 39: 11, 44: 12, 48: 13, 52: 14, 53: 15, 56: 16, 67: 17, 68: 18, 70: 19, 75: 20, 76: 21, 77: 22, 78: 23, 81: 24, 82: 25, 87: 26, 90: 27, 92: 28, 93: 29, 95: 30, 96: 31, 98: 32, 103: 33, 105: 34, 107: 35, 109: 36, 117: 37, 133: 38, 135: 39, 138: 40, 139: 41, 144: 42, 146: 43, 154: 44, 158: 45, 160: 46, 165: 47, 166: 48, 177: 49, 185: 50, 188: 51, 189: 52, 192: 53, 193: 54, 194: 55, 201: 56, 207: 57, 209: 58, 216: 59, 217: 60, 218: 61, 221: 62, 223: 63, 226: 64, 227: 65, 228: 66, 230: 67, 234: 68, 240: 69, 242: 70, 244: 71, 246: 72, 248: 73, 249: 74, 250: 75, 251: 76, 252: 77, 253: 78, 254: 79, 256: 80, 257: 81, 267: 82, 268: 83, 269: 84, 272: 85, 282: 86, 283: 87, 289: 88, 296: 89, 299: 90, 300: 91, 306: 92, 307: 93, 313: 94, 320: 95, 323: 96, 324: 97, 328: 98, 333: 99, 337: 100, 338: 101, 339: 102, 340: 103, 341: 104, 344: 105, 350: 106, 351: 107, 352: 108, 353: 109, 356: 110, 361: 111, 364: 112, 365: 113, 367: 114, 368: 115, 371: 116, 385: 117, 389: 118, 390: 119, 395: 120, 396: 121, 400: 122, 401: 123, 402: 124, 404: 125, 405: 126, 412: 127, 415: 128, 417: 129, 419: 130, 420: 131, 421: 132, 424: 133, 425: 134, 429: 135, 434: 136, 437: 137, 439: 138, 440: 139, 442: 140, 443: 141, 446: 142, 451: 143, 453: 144, 456: 145, 458: 146, 459: 147, 461: 148, 464: 149, 469: 150, 470: 151, 475: 152, 477: 153, 479: 154, 483: 155, 484: 156, 494: 157, 495: 158, 497: 159, 498: 160, 501: 161, 502: 162, 507: 163, 509: 164, 512: 165, 515: 166, 520: 167, 521: 168, 522: 169, 524: 170, 525: 171, 528: 172, 529: 173, 530: 174, 532: 175, 535: 176, 537: 177, 538: 178, 540: 179, 544: 180, 550: 181, 551: 182, 557: 183, 560: 184, 561: 185, 562: 186, 564: 187, 566: 188, 575: 189, 580: 190, 581: 191, 582: 192, 595: 193, 596: 194, 603: 195, 605: 196, 607: 197, 611: 198, 615: 199, 620: 200, 621: 201, 624: 202, 625: 203, 629: 204, 630: 205, 634: 206, 637: 207, 638: 208, 644: 209, 645: 210, 646: 211, 653: 212, 656: 213, 660: 214, 661: 215, 675: 216, 683: 217, 684: 218, 686: 219, 687: 220, 692: 221, 695: 222, 696: 223, 700: 224, 702: 225, 704: 226, 707: 227, 711: 228, 714: 229, 723: 230, 724: 231, 733: 232, 734: 233, 738: 234, 741: 235, 743: 236, 754: 237, 756: 238, 759: 239, 763: 240, 768: 241, 774: 242, 776: 243, 781: 244, 782: 245, 784: 246, 792: 247, 793: 248, 794: 249, 799: 250, 801: 251, 805: 252, 810: 253, 815: 254, 821: 255, 823: 256, 827: 257, 830: 258, 834: 259, 843: 260, 853: 261, 857: 262, 858: 263, 859: 264, 861: 265, 862: 266, 865: 267, 867: 268, 875: 269, 876: 270, 877: 271, 884: 272, 885: 273, 886: 274, 887: 275, 888: 276, 892: 277, 895: 278, 896: 279, 908: 280, 919: 281, 924: 282, 926: 283, 927: 284, 929: 285, 931: 286, 940: 287, 946: 288, 948: 289, 950: 290, 951: 291, 952: 292, 953: 293, 961: 294, 970: 295, 976: 296, 978: 297, 979: 298, 983: 299, 985: 300, 988: 301, 994: 302, 999: 303, 1001: 304, 1003: 305, 1004: 306, 1007: 307, 1025: 308, 1028: 309, 1029: 310, 1034: 311, 1037: 312, 1039: 313, 1043: 314, 1044: 315, 1045: 316, 1047: 317, 1048: 318, 1050: 319, 1052: 320, 1054: 321, 1058: 322, 1061: 323, 1066: 324, 1069: 325, 1070: 326, 1073: 327, 1080: 328, 1082: 329, 1087: 330, 1088: 331, 1089: 332, 1092: 333, 1093: 334, 1097: 335, 1099: 336, 1102: 337, 1103: 338, 1104: 339, 1105: 340, 1118: 341, 1121: 342, 1125: 343, 1127: 344, 1129: 345, 1130: 346, 1135: 347, 1137: 348, 1138: 349, 1145: 350, 1149: 351, 1151: 352, 1153: 353, 1154: 354, 1155: 355, 1156: 356, 1165: 357, 1167: 358, 1168: 359, 1174: 360, 1177: 361, 1179: 362, 1180: 363, 1187: 364, 1191: 365, 1199: 366, 1202: 367, 1203: 368, 1204: 369, 1205: 370, 1206: 371, 1208: 372, 1215: 373, 1220: 374, 1222: 375, 1224: 376, 1226: 377, 1229: 378, 1233: 379, 1235: 380, 1244: 381, 1251: 382, 1256: 383, 1260: 384, 1264: 385, 1266: 386, 1273: 387, 1275: 388, 1282: 389, 1286: 390, 1297: 391, 1299: 392, 1300: 393, 1301: 394, 1305: 395, 1306: 396, 1310: 397, 1316: 398, 1318: 399, 1320: 400, 1321: 401, 1324: 402, 1330: 403, 1331: 404, 1339: 405, 1341: 406, 1344: 407, 1354: 408, 1359: 409, 1360: 410, 1364: 411, 1368: 412, 1371: 413, 1372: 414, 1373: 415, 1376: 416, 1382: 417, 1386: 418, 1388: 419, 1389: 420, 1390: 421, 1392: 422, 1394: 423, 1395: 424, 1397: 425, 1399: 426, 1404: 427, 1406: 428, 1407: 429, 1408: 430, 1411: 431, 1412: 432, 1413: 433, 1421: 434, 1423: 435, 1424: 436, 1426: 437, 1429: 438, 1432: 439, 1439: 440, 1440: 441, 1445: 442, 1447: 443, 1448: 444, 1452: 445, 1455: 446, 1456: 447, 1462: 448, 1463: 449, 1465: 450, 1472: 451, 1474: 452, 1476: 453, 1477: 454, 1480: 455, 1490: 456, 1493: 457, 1495: 458, 1496: 459, 1498: 460, 1508: 461, 1517: 462, 1519: 463, 1523: 464, 1524: 465, 1527: 466, 1531: 467, 1537: 468, 1540: 469, 1543: 470, 1549: 471, 1551: 472, 1553: 473, 1554: 474, 1560: 475, 1565: 476, 1571: 477, 1572: 478, 1576: 479, 1585: 480, 1587: 481, 1589: 482, 1590: 483, 1591: 484, 1593: 485, 1595: 486, 1597: 487, 1599: 488, 1606: 489, 1609: 490, 1612: 491, 1614: 492, 1618: 493, 1619: 494, 1620: 495, 1621: 496, 1626: 497, 1628: 498, 1630: 499, 1632: 500, 1636: 501, 1640: 502, 1643: 503, 1646: 504, 1648: 505, 1650: 506, 1654: 507, 1656: 508, 1658: 509, 1661: 510, 1662: 511, 1665: 512, 1670: 513, 1676: 514, 1678: 515, 1679: 516, 1680: 517, 1682: 518, 1683: 519, 1686: 520, 1687: 521, 1693: 522, 1695: 523, 1696: 524, 1699: 525, 1700: 526, 1702: 527, 1707: 528, 1708: 529, 1713: 530, 1720: 531, 1721: 532, 1725: 533, 1727: 534, 1729: 535, 1730: 536, 1735: 537, 1745: 538, 1747: 539, 1748: 540, 1751: 541, 1753: 542, 1756: 543, 1758: 544, 1759: 545, 1763: 546, 1769: 547, 1770: 548, 1773: 549, 1776: 550, 1779: 551, 1780: 552, 1791: 553, 1794: 554, 1798: 555, 1805: 556, 1807: 557, 1809: 558, 1814: 559, 1827: 560, 1828: 561, 1831: 562, 1835: 563, 1837: 564, 1840: 565, 1844: 566, 1845: 567, 1853: 568, 1855: 569, 1858: 570, 1859: 571, 1860: 572, 1862: 573, 1864: 574, 1867: 575, 1873: 576, 1876: 577, 1880: 578, 1887: 579, 1889: 580, 1890: 581, 1891: 582, 1893: 583, 1898: 584, 1899: 585, 1900: 586, 1903: 587, 1905: 588, 1908: 589, 1910: 590, 1911: 591, 1916: 592, 1919: 593, 1921: 594, 1922: 595, 1925: 596, 1926: 597, 1931: 598, 1933: 599, 1934: 600, 1935: 601, 1938: 602, 1941: 603, 1947: 604, 1950: 605, 1952: 606, 1955: 607, 1960: 608, 1961: 609, 1963: 610, 1964: 611, 1968: 612, 1969: 613, 1973: 614, 1976: 615, 1977: 616, 1981: 617, 1983: 618, 1984: 619, 1989: 620, 1993: 621, 1994: 622, 1995: 623, 1997: 624, 2004: 625, 2006: 626, 2009: 627, 2012: 628, 2013: 629, 2016: 630, 2020: 631, 2026: 632, 2027: 633, 2031: 634, 2033: 635, 2044: 636, 2050: 637, 2052: 638, 2053: 639, 2054: 640, 2056: 641, 2057: 642, 2061: 643, 2065: 644}
class0_reverse = {value: i for i, value in class0.items()}
class1_reverse = {value: i for i, value in class1.items()}
class2_reverse = {value: i for i, value in class2.items()}
class3_reverse = {value: i for i, value in class3.items()}
class4_reverse = {value: i for i, value in class4.items()}
class Net(torch.nn.Module):
    def __init__(self):
        super().__init__()

        self.conv1 = GraphConv(dataset.num_features, 128)
        self.pool1 = TopKPooling(128, ratio=0.8)
        self.conv2 = GraphConv(128, 128)
        self.pool2 = TopKPooling(128, ratio=0.8)
        self.conv3 = GraphConv(128, 128)
        self.pool3 = TopKPooling(128, ratio=0.8)

        self.lin1 = torch.nn.Linear(256, 128)
        self.lin2 = torch.nn.Linear(128, 64)
        self.lin3 = torch.nn.Linear(64, dataset.num_classes)

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch

        x = F.relu(self.conv1(x, edge_index))
        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)
        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)

        x = F.relu(self.conv2(x, edge_index))
        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)
        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)

        x = F.relu(self.conv3(x, edge_index))
        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)
        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)

        x = x1 + x2 + x3

        x = F.relu(self.lin1(x))
        x = F.dropout(x, p=0.5, training=self.training)
        x = F.relu(self.lin2(x))
        x = F.log_softmax(self.lin3(x), dim=-1)

        return x


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Net().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)


def train(epoch):
    model.train()

    loss_all = 0
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, data.y)
        loss.backward()
        loss_all += data.num_graphs * loss.item()
        optimizer.step()
    return loss_all / len(train_dataset)


# def test(loader):
#     model.eval()
#
#     correct = 0
#     for data in loader:
#         data = data.to(device)
#         pred = model(data).max(dim=1)[1]
#         correct += pred.eq(data.y).sum().item()
#     return correct / len(loader.dataset)
def test(loader):
    model.eval()

    correct = 0
    labels = []
    preds = []
    for data in loader:
        data = data.to(device)
        pred = model(data).max(dim=1)[1]
        correct += pred.eq(data.y).sum().item()

        labels.append(np.squeeze(data['y'].data.numpy()))
        preds.append(pred.cpu().data.numpy())
    labels = np.hstack(labels)
    preds = np.hstack(preds)
    # print("labels:", labels)
    # print("preds:", preds)
    # real_labels = []
    # for n in labels:
    #     real_labels.append(class0_reverse[n])
    # real_preds = []
    # for m in preds:
    #     real_preds.append(class0_reverse[m])
    # print('reallabels:', real_labels)
    # print('realpreds:', real_preds)
    read_dic = np.load(bmname+"_short_path.npy", allow_pickle=True).item()
    # print(read_dic[2][3])
    distance = []
    for i in range(len(labels)):
        a = read_dic[labels[i]][preds[i]]
        distance.append(a)
    #print(distance)
    result_dis = {}
    sum_dis = 0
    for i in set(distance):
        result_dis[i] = distance.count(i)
        sum_dis = i * result_dis[i] + sum_dis
    average_dis = sum_dis / len(labels)
    return correct / len(loader.dataset), average_dis

for epoch in range(1, 51):
    loss = train(epoch)
    train_acc, train_GCN = test(train_loader)
    test_acc, test_GCN = test(test_loader)
    print(f'Epoch: {epoch:03d}, Loss: {loss:.5f}, Train Acc: {train_acc:.5f}, '
          f'Test Acc: {test_acc:.5f}, Train_GCN:{train_GCN:.4f}, Test_GCN:{test_GCN:.4f}')

torch.save(model.state_dict(), 'model/' + bmname + sname + '_topk_pool.pt')